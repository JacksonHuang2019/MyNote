##一个线程罢工的诡异事件
![](https://i.loli.net/2019/07/19/5d313f4a3a31f18582.jpg)
## 背景
事情（事故）是这样的，突然收到报警，线上某个应用里业务逻辑没有执行，导致的结果是数据库里的某些数据没有更新。

虽然是前人写的代码，但作为 Bug maker&killer 只能咬着牙上了。

因为之前没有接触过出问题这块的逻辑，所以简单理了下如图：

![](https://i.loli.net/2019/07/19/5d313f4c9d69456679.jpg)
1. 有一个生产线程一直源源不断的往队列写数据。
2. 消费线程也一直不停的取出数据后写入后续的业务线程池。
3. 业务线程池里的线程会对每个任务进行入库操作。

整个过程还是比较清晰的，就是一个典型的生产者消费者模型。

### 尝试定位
接下来便是尝试定位这个问题，首先例行检查了以下几项：
1. 是否内存有内存溢出
2. 应用 GC 是否有异常？

通过日志以及监控发现以上两项都是正常的。

紧接着便 dump 了线程快照查看业务线程池中的线程都在干啥
![](https://i.loli.net/2019/07/19/5d313f4f2a5fc61091.jpg)

结果发现所有业务线程池都处于 waiting 状态，队列也是空的。

同时生产者使用的队列却已经满了，没有任何消费迹象。

结合上面的流程图不难发现应该是消费队列的 Consumer 出问题了，导致上游的队列不能消费，下有的业务线程池没事可做。

### review 代码
于是查看了消费代码的业务逻辑，同时也发现消费线程是一个单线程。

![](https://i.loli.net/2019/07/19/5d313f5162ec253903.jpg)

结合之前的线程快照，我发现这个消费线程也是处于 waiting 状态，和后面的业务线程池一模一样。

他做的事情基本上就是对消息解析，之后丢到后面的业务线程池中，没有发现什么特别的地方。
> 但是由于里面的分支特别多（switch case），看着有点头疼；所以我与写这个业务代码的同学沟通后他告诉我确
实也只是入口处解析了一下数据，后续所有的业务逻辑都是丢到线程池中处理的，于是我便带着这个前提去排查了（埋下了伏笔）。

因为这里消费的队列其实是一个 disruptor 队列；它和我们常用的 BlockQueue 不太一样，不是由开发者自定义一个消费逻辑进行处理的；而是在初始化队列时直接丢一个线程池进去，它会在内部使用这个线程池进行消费，同时回调一个方法，在这个方法里我们写自己的消费逻辑。

所以对于开发者而言，这个消费逻辑其实是一个黑盒。

于是在我反复 review 了消费代码中的数据解析逻辑发现不太可能出现问题后，便开始疯狂怀疑是不是 disruptor 自身的问题导致这个消费线程罢工了。

再翻了一阵 disruptor 的源码后依旧没发现什么问题后我咨询对 disruptor 较熟的@咖啡拿铁，在他的帮助下在本地模拟出来和生产一样的情况。

## 本地模拟
![](https://i.loli.net/2019/07/19/5d313f52c634323563.jpg)
